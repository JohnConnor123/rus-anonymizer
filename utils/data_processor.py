"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
"""

import json
import random
from pathlib import Path
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
import logging
import argparse
import glob
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class DatasetStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    total_dialogs: int
    pd_dialogs: int
    non_pd_dialogs: int
    pd_percentage: float
    total_entities: int
    entity_types: Dict[str, int]


class DatasetProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    
    def __init__(self, input_files: List[str], output_dir: str = "data/processed"):
        self.input_files = input_files
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def load_dataset(self, file_path: str) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ñ–∞–π–ª–∞"""
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data.get('dialogs', []))} –¥–∏–∞–ª–æ–≥–æ–≤")
        return data
    
    def calculate_stats(self, dialogs: List[Dict[str, Any]]) -> DatasetStats:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        pd_count = sum(1 for d in dialogs if d.get('has_pd', False))
        total_entities = sum(len(d.get('entities', [])) for d in dialogs)
        
        # –ü–æ–¥—Å—á–µ—Ç —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π
        entity_types = {}
        for dialog in dialogs:
            for entity in dialog.get('entities', []):
                entity_type = entity.get('type', 'UNKNOWN')
                entity_types[entity_type] = entity_types.get(entity_type, 0) + 1
        
        return DatasetStats(
            total_dialogs=len(dialogs),
            pd_dialogs=pd_count,
            non_pd_dialogs=len(dialogs) - pd_count,
            pd_percentage=pd_count / len(dialogs) * 100 if dialogs else 0,
            total_entities=total_entities,
            entity_types=entity_types
        )
    
    def merge_datasets(self) -> Dict[str, Any]:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã"""
        logger.info("–ù–∞—á–∏–Ω–∞—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤")
        
        all_dialogs = []
        all_metadata = []
        total_cost_tokens = 0
        
        for file_path in self.input_files:
            data = self.load_dataset(file_path)
            
            # –°–æ–±–∏—Ä–∞–µ–º –¥–∏–∞–ª–æ–≥–∏
            dialogs = data.get('dialogs', [])
            all_dialogs.extend(dialogs)
            
            # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = data.get('metadata', {})
            all_metadata.append(metadata)
            
            # –°—É–º–º–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã
            gen_stats = metadata.get('generation_stats', {})
            total_cost_tokens += gen_stats.get('total_cost_tokens', 0)
        
        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º ID –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        for i, dialog in enumerate(all_dialogs, 1):
            dialog['id'] = i
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        stats = self.calculate_stats(all_dialogs)
        
        merged_metadata = {
            "total_dialogs": stats.total_dialogs,
            "merge_timestamp": datetime.now().timestamp(),
            "source_files": self.input_files,
            "merged_stats": {
                "total_dialogs": stats.total_dialogs,
                "pd_dialogs": stats.pd_dialogs,
                "non_pd_dialogs": stats.non_pd_dialogs,
                "pd_percentage": stats.pd_percentage,
                "total_entities": stats.total_entities,
                "entity_types": stats.entity_types,
                "total_cost_tokens": total_cost_tokens
            },
            "source_metadata": all_metadata
        }
        
        logger.info(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {stats.total_dialogs} –¥–∏–∞–ª–æ–≥–æ–≤")
        logger.info(f"–î–∏–∞–ª–æ–≥–æ–≤ —Å –ü–î: {stats.pd_dialogs} ({stats.pd_percentage:.1f}%)")
        
        return {
            "metadata": merged_metadata,
            "dialogs": all_dialogs
        }
    
    def split_dataset(self, data: Dict[str, Any], 
                     train_ratio: float = 0.7) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """–†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ train/val"""
        logger.info(f"–†–∞–∑–¥–µ–ª—è—é –¥–∞—Ç–∞—Å–µ—Ç –≤ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ train={train_ratio:.1f}")
        
        dialogs = data['dialogs'].copy()
        random.shuffle(dialogs)
        
        split_idx = int(len(dialogs) * train_ratio)
        train_dialogs = dialogs[:split_idx]
        val_dialogs = dialogs[split_idx:]
        
        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º ID –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞
        for i, dialog in enumerate(train_dialogs, 1):
            dialog['id'] = i
            
        for i, dialog in enumerate(val_dialogs, 1):
            dialog['id'] = i
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è train
        train_stats = self.calculate_stats(train_dialogs)
        train_metadata = data['metadata'].copy()
        train_metadata.update({
            "split": "train",
            "split_timestamp": datetime.now().timestamp(),
            "split_ratio": train_ratio,
            "total_dialogs": train_stats.total_dialogs,
            "pd_dialogs": train_stats.pd_dialogs,
            "pd_percentage": train_stats.pd_percentage,
            "total_entities": train_stats.total_entities,
            "entity_types": train_stats.entity_types
        })
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è val
        val_stats = self.calculate_stats(val_dialogs)
        val_metadata = data['metadata'].copy()
        val_metadata.update({
            "split": "val",
            "split_timestamp": datetime.now().timestamp(),
            "split_ratio": 1 - train_ratio,
            "total_dialogs": val_stats.total_dialogs,
            "pd_dialogs": val_stats.pd_dialogs,
            "pd_percentage": val_stats.pd_percentage,
            "total_entities": val_stats.total_entities,
            "entity_types": val_stats.entity_types
        })
        
        train_data = {
            "metadata": train_metadata,
            "dialogs": train_dialogs
        }
        
        val_data = {
            "metadata": val_metadata,
            "dialogs": val_dialogs
        }
        
        logger.info(f"Train: {len(train_dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤")
        logger.info(f"Val: {len(val_dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤")
        
        return train_data, val_data
    
    def save_dataset(self, data: Dict[str, Any], filename: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–∞–π–ª"""
        output_path = self.output_dir / filename
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"–î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    
    def process_all(self, train_ratio: float = 0.7, seed: int = 42):
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ"""
        random.seed(seed)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
        merged_data = self.merge_datasets()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        self.save_dataset(merged_data, "merged_dataset.json")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
        train_data, val_data = self.split_dataset(merged_data, train_ratio)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
        self.save_dataset(train_data, "train_dataset.json")
        self.save_dataset(val_data, "val_dataset.json")
        
        return {
            "merged": merged_data,
            "train": train_data,
            "val": val_data
        }


def analyze_dataset(file_path: str):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    processor = DatasetProcessor([file_path])
    data = processor.load_dataset(file_path)
    stats = processor.calculate_stats(data.get('dialogs', []))
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {file_path}")
    print(f"–í—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–æ–≤: {stats.total_dialogs}")
    print(f"–î–∏–∞–ª–æ–≥–æ–≤ —Å –ü–î: {stats.pd_dialogs} ({stats.pd_percentage:.1f}%)")
    print(f"–î–∏–∞–ª–æ–≥–æ–≤ –±–µ–∑ –ü–î: {stats.non_pd_dialogs}")
    print(f"–í—Å–µ–≥–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {stats.total_entities}")
    
    if stats.entity_types:
        print("\n–¢–∏–ø—ã —Å—É—â–Ω–æ—Å—Ç–µ–π:")
        for entity_type, count in sorted(stats.entity_types.items()):
            print(f"  {entity_type}: {count}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤")
    parser.add_argument('--input', help='–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞')
    parser.add_argument('--merge', nargs='+', help='–§–∞–π–ª—ã –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è (–º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å glob)')
    parser.add_argument('--output', help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')
    parser.add_argument('--analyze-only', action='store_true', help='–¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è')
    parser.add_argument('--train-ratio', type=float, default=0.7, help='–î–æ–ª—è train –¥–∞—Ç–∞—Å–µ—Ç–∞')
    
    args = parser.parse_args()
    
    if args.analyze_only and args.input:
        analyze_dataset(args.input)
        return
        
    if args.merge:
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –ø–∞—Ç—Ç–µ—Ä–Ω glob, —Ä–∞—Å—à–∏—Ä—è–µ–º –µ–≥–æ
        files = []
        for pattern in args.merge:
            if '*' in pattern or '?' in pattern:
                files.extend(glob.glob(pattern))
            else:
                files.append(pattern)
        
        if not files:
            print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è")
            return
            
        processor = DatasetProcessor(files)
        merged_data = processor.merge_datasets()
        
        if args.output:
            processor.save_dataset(merged_data, Path(args.output).name)
            print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {args.output}")
        else:
            print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã (—Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω)")
            
    elif args.input:
        processor = DatasetProcessor([args.input])
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        print(f"üìÅ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {args.input}")
        analyze_dataset(args.input)
    else:
        parser.print_help()


if __name__ == "__main__":
    main() 