#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–æ–≤.
–ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫ –∏ –±—É–¥—É—â–∏—Ö –ø—Ä–æ–≥–æ–Ω–æ–≤.
"""

import json
import time
import sys
import argparse
import csv
from pathlib import Path
from typing import Dict, List, Tuple, Any
from collections import defaultdict
import os

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# –ò–º–ø–æ—Ä—Ç—ã –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
from anonymizers.natasha_enhanced.enhanced_natasha import EnhancedNatashaAnonymizer
from anonymizers.spacy_extended.spacy_anonymizer import SpacyAnonymizer
from anonymizers.transformers_bert.bert_anonymizer import BertAnonymizer
from anonymizers.hybrid_advanced.hybrid_anonymizer import HybridAdvancedAnonymizer
from anonymizers.regexp_baseline.regexp_anonymizer import RegExpBaselineAnonymizer


def load_test_dataset(dataset_path: str = None) -> List[Dict]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ JSON —Ñ–∞–π–ª–∞.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å metadata –∏ dialogs.
    """
    test_file_path = dataset_path
    
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {test_file_path}")
    
    try:
        with open(test_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {test_file_path}: {e}")
        return []

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å metadata
    if isinstance(data, dict) and 'dialogs' in data:
        print("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ó–∞–≥—Ä—É–∑–∫–∞...")
        
        source_dialogs = data.get('dialogs', [])
        converted_dialogs = []

        for dialog in source_dialogs:
            entities = []
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–≤–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø–æ–ª—è –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π: 'entities' –∏ 'personal_data'
            entities_source = dialog.get('entities', dialog.get('personal_data', []))
            
            for entity_data in entities_source:
                # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—É—â–Ω–æ—Å—Ç—å
                entity = {
                    'start': entity_data.get('start'),
                    'end': entity_data.get('end'),
                    'type': entity_data.get('type'),
                }
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å—É—â–Ω–æ—Å—Ç–∏
                if 'text' in entity_data:
                    entity['text'] = entity_data['text']
                elif 'value' in entity_data:
                    entity['text'] = entity_data['value']
                else:
                    entity['text'] = ''
                
                entities.append(entity)

            converted_dialogs.append({
                'id': dialog.get('id', len(converted_dialogs) + 1),
                'text': dialog.get('text', ''),
                'entities': entities,
                'has_pd': bool(entities),
                'description': dialog.get('description', f"–î–∏–∞–ª–æ–≥ ID: {dialog.get('id')}")
            })
        
        print(f"üëç –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. {len(converted_dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤ –≥–æ—Ç–æ–≤–æ –∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.")
        return converted_dialogs

    print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ —Ñ–∞–π–ª–µ {test_file_path}")
    return []


def calculate_overlap_score(entity1: Tuple[int, int, str], entity2: Tuple[int, int, str]) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–µ–ø–µ–Ω—å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –¥–≤—É—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (0.0 - 1.0)."""
    start1, end1, type1 = entity1
    start2, end2, type2 = entity2
    
    if type1 != type2:
        return 0.0
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
    overlap_start = max(start1, start2)
    overlap_end = min(end1, end2)
    
    if overlap_start >= overlap_end:
        return 0.0
    
    overlap_length = overlap_end - overlap_start
    union_length = max(end1, end2) - min(start1, start2)
    
    return overlap_length / union_length if union_length > 0 else 0.0


def merge_person_entities(entities: List[Tuple[int, int, str, str]]) -> List[Tuple[int, int, str, str]]:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ PERSON –≤ –æ–¥–Ω—É."""
    if not entities:
        return entities
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏
    sorted_entities = sorted(entities, key=lambda x: x[0])
    merged = []
    
    i = 0
    while i < len(sorted_entities):
        start, end, entity_type, value = sorted_entities[i]
        
        if entity_type == 'PERSON':
            # –ò—â–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ PERSON —Å—É—â–Ω–æ—Å—Ç–∏
            merged_start = start
            merged_end = end
            merged_value = value
            
            j = i + 1
            while j < len(sorted_entities):
                next_start, next_end, next_type, next_value = sorted_entities[j]
                
                # –ï—Å–ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—É—â–Ω–æ—Å—Ç—å —Ç–æ–∂–µ PERSON –∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ä—è–¥–æ–º (—Å —É—á–µ—Ç–æ–º –ø—Ä–æ–±–µ–ª–æ–≤)
                if (next_type == 'PERSON' and 
                    next_start <= merged_end + 3):  # +3 –¥–ª—è —É—á–µ—Ç–∞ –ø—Ä–æ–±–µ–ª–æ–≤
                    merged_end = next_end
                    merged_value = merged_value + " " + next_value if not merged_value.endswith(next_value) else merged_value
                    j += 1
                else:
                    break
            
            merged.append((merged_start, merged_end, entity_type, merged_value.strip()))
            i = j
        else:
            merged.append((start, end, entity_type, value))
            i += 1
    
    return merged


def smart_entity_matching(true_entities: List[Dict], predicted_entities: List[Dict], 
                         overlap_threshold: float = 0.2) -> Tuple[int, int, int]:
    """
    –£–º–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å —É—á–µ—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ, –≤—Å–µ–≥–æ_–∏—Å—Ç–∏–Ω–Ω—ã—Ö, –≤—Å–µ–≥–æ_–Ω–∞–π–¥–µ–Ω–Ω—ã—Ö).
    """
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
    true_set = [(e['start'], e['end'], e['type']) for e in true_entities]
    pred_set = [(e['start'], e['end'], e['type']) for e in predicted_entities]
    
    matched_true = set()
    matched_pred = set()
    
    # –ò—â–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    exact_matches = set(true_set) & set(pred_set)
    matched_true.update(exact_matches)
    matched_pred.update(exact_matches)
    
    # –ò—â–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –¥–ª—è –Ω–µ—Å–æ–≤–ø–∞–≤—à–∏—Ö
    remaining_true = [e for e in true_set if e not in matched_true]
    remaining_pred = [e for e in pred_set if e not in matched_pred]
    
    for true_entity in remaining_true:
        best_match = None
        best_score = 0.0
        
        for pred_entity in remaining_pred:
            if pred_entity in matched_pred:
                continue
                
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
            score = calculate_overlap_score(true_entity, pred_entity)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –±–ª–∏–∑–∫–∏—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
            start_diff = abs(true_entity[0] - pred_entity[0])
            end_diff = abs(true_entity[1] - pred_entity[1])
            
            # –ï—Å–ª–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ—á–µ–Ω—å –±–ª–∏–∑–∫–∏–µ (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 3 —Å–∏–º–≤–æ–ª–æ–≤), —Å—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º
            if start_diff <= 3 and end_diff <= 3 and true_entity[2] == pred_entity[2]:
                score = max(score, 0.8)  # –í—ã—Å–æ–∫–∏–π –±–∞–ª–ª –¥–ª—è –±–ª–∏–∑–∫–∏—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
            
            if score > best_score and score >= overlap_threshold:
                best_score = score
                best_match = pred_entity
        
        if best_match:
            matched_true.add(true_entity)
            matched_pred.add(best_match)
    
    return len(matched_true), len(true_entities), len(predicted_entities)


def calculate_metrics(true_entities: List[Dict], predicted_entities: List[Dict]) -> Dict[str, float]:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ precision, recall, F1 —Å —É–º–Ω—ã–º —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º."""
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–º–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
    tp, total_true, total_pred = smart_entity_matching(true_entities, predicted_entities)
    
    fp = total_pred - tp  # False Positives
    fn = total_true - tp  # False Negatives
    
    precision = tp / total_pred if total_pred > 0 else 0.0
    recall = tp / total_true if total_true > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'tp': tp,
        'fp': fp,
        'fn': fn
    }


def improve_extracted_entities(extracted: List[Tuple[int, int, str, str]], text: str) -> List[Tuple[int, int, str, str]]:
    """–£–ª—É—á—à–∞–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏."""
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ PERSON —Å—É—â–Ω–æ—Å—Ç–∏
    improved = merge_person_entities(extracted)
    
    # –£–ª—É—á—à–∞–µ–º AGE —Å—É—â–Ω–æ—Å—Ç–∏ - —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞
    final_improved = []
    for start, end, entity_type, value in improved:
        if entity_type == 'AGE':
            # –ò—â–µ–º —á–∏—Å–ª–æ–≤—É—é —á–∞—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
            import re
            age_match = re.search(r'\d+\s*–ª–µ—Ç', value)
            if age_match:
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é —á–∏—Å–ª–æ–≤–æ–π —á–∞—Å—Ç–∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
                age_text = age_match.group()
                age_start = text.find(age_text, start)
                if age_start != -1:
                    age_end = age_start + len(age_text)
                    final_improved.append((age_start, age_end, entity_type, age_text))
                    continue
        
        final_improved.append((start, end, entity_type, value))
    
    return final_improved


def calculate_metrics_by_type(true_entities: List[Dict], predicted_entities: List[Dict], 
                             overlap_threshold: float = 0.2) -> Dict[str, Dict[str, float]]:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ precision, recall, F1, accuracy –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏ –æ—Ç–¥–µ–ª—å–Ω–æ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {entity_type: {metric_name: value}}.
    """
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ —Ç–∏–ø–∞–º
    true_by_type = defaultdict(list)
    pred_by_type = defaultdict(list)
    
    for entity in true_entities:
        true_by_type[entity['type']].append(entity)
    
    for entity in predicted_entities:
        pred_by_type[entity['type']].append(entity)
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã
    all_types = set(true_by_type.keys()) | set(pred_by_type.keys())
    
    metrics_by_type = {}
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
    for entity_type in all_types:
        true_type_entities = true_by_type[entity_type]
        pred_type_entities = pred_by_type[entity_type]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–º–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞
        tp, total_true, total_pred = smart_entity_matching(true_type_entities, pred_type_entities, overlap_threshold)
        
        fp = total_pred - tp
        fn = total_true - tp
        
        precision = tp / total_pred if total_pred > 0 else 0.0
        recall = tp / total_true if total_true > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        accuracy = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0.0
        
        metrics_by_type[entity_type] = {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'accuracy': accuracy,
            'tp': tp,
            'fp': fp,
            'fn': fn,
            'total_true': total_true,
            'total_pred': total_pred
        }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (ALL)
    overall_metrics = calculate_metrics(true_entities, predicted_entities)
    overall_metrics['accuracy'] = overall_metrics['tp'] / (overall_metrics['tp'] + overall_metrics['fp'] + overall_metrics['fn']) if (overall_metrics['tp'] + overall_metrics['fp'] + overall_metrics['fn']) > 0 else 0.0
    overall_metrics['total_true'] = len(true_entities)
    overall_metrics['total_pred'] = len(predicted_entities)
    
    metrics_by_type['ALL'] = overall_metrics
    
    return metrics_by_type


def get_dataset_name(dataset_path: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É."""
    return Path(dataset_path).stem


def save_metrics_to_csv(metrics_by_type: Dict[str, Dict[str, float]], 
                       method_name: str, dataset_name: str, output_dir: Path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –≤ CSV —Ñ–∞–π–ª."""
    # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
    safe_method_name = method_name.replace(" ", "_").replace("/", "_")
    filename = f"{dataset_name}_{safe_method_name}_metrics.csv"
    filepath = output_dir / filename
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['entity_type', 'precision', 'recall', 'f1', 'accuracy', 'tp', 'fp', 'fn', 'total_true', 'total_pred']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        for entity_type, metrics in metrics_by_type.items():
            row = {'entity_type': entity_type}
            row.update(metrics)
            writer.writerow(row)
    
    print(f"üíæ –ú–µ—Ç—Ä–∏–∫–∏ {method_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")


def save_combined_metrics_to_csv(all_results: Dict[str, Dict[str, Dict[str, float]]], 
                                dataset_name: str, output_dir: Path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤—Å–µ—Ö –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ –æ–¥–∏–Ω CSV —Ñ–∞–π–ª."""
    filename = f"{dataset_name}_combined_metrics.csv"
    filepath = output_dir / filename
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['method', 'entity_type', 'precision', 'recall', 'f1', 'accuracy', 'tp', 'fp', 'fn', 'total_true', 'total_pred']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        for method_name, metrics_by_type in all_results.items():
            for entity_type, metrics in metrics_by_type.items():
                row = {
                    'method': method_name,
                    'entity_type': entity_type
                }
                row.update(metrics)
                writer.writerow(row)
    
    print(f"üíæ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")


def validate_anonymizer(anonymizer, name: str, test_data: List[Dict]) -> Tuple[Dict[str, Any], Dict[str, Dict[str, float]]]:
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –æ–¥–∏–Ω –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º."""
    print(f"\nüîç –í–∞–ª–∏–¥–∞—Ü–∏—è {name}...")
    
    start_time = time.time()
    
    all_true_entities = []
    all_predicted_entities = []
    processing_times = []
    
    for i, example in enumerate(test_data):
        text = example['text']
        true_entities = example['entities']
        
        # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        process_start = time.time()
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
            extracted = anonymizer.extract_entities(text)
            
            # –£–ª—É—á—à–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
            improved_extracted = improve_extracted_entities(extracted, text)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            predicted_entities = []
            for start, end, entity_type, value in improved_extracted:
                predicted_entities.append({
                    'start': start,
                    'end': end,
                    'type': entity_type,
                    'value': value
                })
            
            all_true_entities.extend(true_entities)
            all_predicted_entities.extend(predicted_entities)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–∏–º–µ—Ä–µ {i+1}: {e}")
            continue
        
        process_time = time.time() - process_start
        processing_times.append(process_time)
    
    total_time = time.time() - start_time
    
    # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    overall_metrics = calculate_metrics(all_true_entities, all_predicted_entities)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º
    metrics_by_type = calculate_metrics_by_type(all_true_entities, all_predicted_entities)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ –æ–±—â–∏–º –º–µ—Ç—Ä–∏–∫–∞–º
    overall_metrics.update({
        'total_time': total_time,
        'avg_time_per_example': sum(processing_times) / len(processing_times) if processing_times else 0,
        'examples_processed': len(processing_times)
    })
    
    return overall_metrics, metrics_by_type


def print_results(results: Dict[str, Dict[str, Any]]):
    """–í—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–æ–Ω—Å–æ–ª—å."""
    print("\n" + "="*80)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ê–õ–ò–î–ê–¶–ò–ò")
    print("="*80)
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
    print(f"{'–ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä':<25} {'Precision':<10} {'Recall':<10} {'F1':<10} {'–í—Ä–µ–º—è (—Å)':<12}")
    print("-" * 80)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ F1 score
    sorted_results = sorted(results.items(), key=lambda x: x[1]['f1'], reverse=True)
    
    for name, metrics in sorted_results:
        print(f"{name:<25} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} "
              f"{metrics['f1']:<10.3f} {metrics['total_time']:<12.2f}")
    
    print("-" * 80)
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ª—É—á—à–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
    best_name, best_metrics = sorted_results[0]
    print(f"\nüèÜ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_name}")
    print(f"   F1-Score: {best_metrics['f1']:.3f}")
    print(f"   Precision: {best_metrics['precision']:.3f}")
    print(f"   Recall: {best_metrics['recall']:.3f}")
    print(f"   TP: {best_metrics['tp']}, FP: {best_metrics['fp']}, FN: {best_metrics['fn']}")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: {best_metrics['avg_time_per_example']*1000:.1f}–º—Å")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser(
        description="–ü—Ä–æ—Å—Ç–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python validate_simple.py
  python validate_simple.py --dataset test_dataset.json
  python validate_simple.py --dataset data/generated/deepseek_v3_dataset_manual_final.json
  python validate_simple.py -d /path/to/custom_dataset.json
        """
    )
    
    parser.add_argument(
        '--dataset', '-d',
        type=str,
        required=True,
        help='–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏.'
    )
    
    args = parser.parse_args()
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤")
    print(f"üìÅ –†–∞–±–æ—á–∞—è –ø–∞–ø–∫–∞: {Path.cwd()}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_data = load_test_dataset(args.dataset)
    if not test_data:
        print("‚ùå –ù–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
        return
    
    print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_name = get_dataset_name(args.dataset)
    print(f"üìä –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_name}")
    
    # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–æ–≤
    output_dir = Path("data/reports")
    
    # –°–ø–∏—Å–æ–∫ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    anonymizers = {
        "RegExp Baseline": RegExpBaselineAnonymizer(),
        "Natasha Enhanced": EnhancedNatashaAnonymizer(),
        "SpaCy Extended": SpacyAnonymizer(),
        "Transformers BERT": BertAnonymizer(),
        "Hybrid Advanced": HybridAdvancedAnonymizer(),
    }
    
    print(f"üéØ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(anonymizers)} –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤")
    
    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä
    results = {}
    all_detailed_results = {}
    
    for name, anonymizer in anonymizers.items():
        try:
            overall_metrics, metrics_by_type = validate_anonymizer(anonymizer, name, test_data)
            results[name] = overall_metrics
            all_detailed_results[name] = metrics_by_type
            
            print(f"‚úÖ {name}: F1={overall_metrics['f1']:.3f}, P={overall_metrics['precision']:.3f}, R={overall_metrics['recall']:.3f}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π CSV
            save_metrics_to_csv(metrics_by_type, name, dataset_name, output_dir)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ {name}: {e}")
            continue
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    if all_detailed_results:
        save_combined_metrics_to_csv(all_detailed_results, dataset_name, output_dir)
    
    # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if results:
        print_results(results)
        print(f"\nüìÅ –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É: {output_dir}")
        print(f"üìÑ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {len(anonymizers) + 1}")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏")


if __name__ == "__main__":
    main()