#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–æ–≤.
–ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫ –∏ –±—É–¥—É—â–∏—Ö –ø—Ä–æ–≥–æ–Ω–æ–≤.
"""

import json
import time
import sys
import argparse
import csv
from pathlib import Path
from typing import Dict, List, Tuple, Any
from collections import defaultdict
import os

# –ò–º–ø–æ—Ä—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–±–∞—Ä–∞
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    print("üì¶ tqdm –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tqdm")
    TQDM_AVAILABLE = False

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# –ò–º–ø–æ—Ä—Ç—ã –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
from anonymizers.natasha_enhanced.enhanced_natasha import EnhancedNatashaAnonymizer
from anonymizers.spacy_extended.spacy_anonymizer import SpacyAnonymizer
from anonymizers.transformers_bert.bert_anonymizer import BertAnonymizer
from anonymizers.hybrid_advanced.hybrid_anonymizer import HybridAdvancedAnonymizer
from anonymizers.regexp_baseline.regexp_anonymizer import RegExpBaselineAnonymizer

# –ò–º–ø–æ—Ä—Ç –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–≥–æ –º–æ–¥—É–ª—è –º–µ—Ç—Ä–∏–∫
from utils.metrics import calculate_metrics, calculate_metrics_by_type


def load_test_dataset(dataset_path: str = None) -> List[Dict]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ JSON —Ñ–∞–π–ª–∞.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å metadata –∏ dialogs.
    """
    test_file_path = dataset_path
    
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {test_file_path}")
    
    try:
        with open(test_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {test_file_path}: {e}")
        return []

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å metadata
    if isinstance(data, dict) and 'dialogs' in data:
        print("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ó–∞–≥—Ä—É–∑–∫–∞...")
        
        source_dialogs = data.get('dialogs', [])
        converted_dialogs = []

        for dialog in source_dialogs:
            entities = []
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–≤–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø–æ–ª—è –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π: 'entities' –∏ 'personal_data'
            entities_source = dialog.get('entities', dialog.get('personal_data', []))
            
            for entity_data in entities_source:
                # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—É—â–Ω–æ—Å—Ç—å
                entity = {
                    'start': entity_data.get('start'),
                    'end': entity_data.get('end'),
                    'type': entity_data.get('type'),
                }
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å—É—â–Ω–æ—Å—Ç–∏
                if 'text' in entity_data:
                    entity['text'] = entity_data['text']
                elif 'value' in entity_data:
                    entity['text'] = entity_data['value']
                else:
                    entity['text'] = ''
                
                entities.append(entity)

            converted_dialogs.append({
                'id': dialog.get('id', len(converted_dialogs) + 1),
                'text': dialog.get('text', ''),
                'entities': entities,
                'has_pd': bool(entities),
                'description': dialog.get('description', f"–î–∏–∞–ª–æ–≥ ID: {dialog.get('id')}")
            })
        
        print(f"üëç –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. {len(converted_dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤ –≥–æ—Ç–æ–≤–æ –∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.")
        return converted_dialogs

    print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ —Ñ–∞–π–ª–µ {test_file_path}")
    return []


def merge_person_entities(entities: List[Tuple[int, int, str, str]]) -> List[Tuple[int, int, str, str]]:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ PERSON –≤ –æ–¥–Ω—É."""
    if not entities:
        return entities
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏
    sorted_entities = sorted(entities, key=lambda x: x[0])
    merged = []
    
    i = 0
    while i < len(sorted_entities):
        start, end, entity_type, value = sorted_entities[i]
        
        if entity_type == 'PERSON':
            # –ò—â–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ PERSON —Å—É—â–Ω–æ—Å—Ç–∏
            merged_start = start
            merged_end = end
            merged_value = value
            
            j = i + 1
            while j < len(sorted_entities):
                next_start, next_end, next_type, next_value = sorted_entities[j]
                
                # –ï—Å–ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—É—â–Ω–æ—Å—Ç—å —Ç–æ–∂–µ PERSON –∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ä—è–¥–æ–º (—Å —É—á–µ—Ç–æ–º –ø—Ä–æ–±–µ–ª–æ–≤)
                if (next_type == 'PERSON' and 
                    next_start <= merged_end + 3):  # +3 –¥–ª—è —É—á–µ—Ç–∞ –ø—Ä–æ–±–µ–ª–æ–≤
                    merged_end = next_end
                    merged_value = merged_value + " " + next_value if not merged_value.endswith(next_value) else merged_value
                    j += 1
                else:
                    break
            
            merged.append((merged_start, merged_end, entity_type, merged_value.strip()))
            i = j
        else:
            merged.append((start, end, entity_type, value))
            i += 1
    
    return merged


def improve_extracted_entities(extracted: List[Tuple[int, int, str, str]], text: str) -> List[Tuple[int, int, str, str]]:
    """–£–ª—É—á—à–∞–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏."""
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ PERSON —Å—É—â–Ω–æ—Å—Ç–∏
    improved = merge_person_entities(extracted)
    
    # –£–ª—É—á—à–∞–µ–º AGE —Å—É—â–Ω–æ—Å—Ç–∏ - —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞
    final_improved = []
    for start, end, entity_type, value in improved:
        if entity_type == 'AGE':
            # –ò—â–µ–º —á–∏—Å–ª–æ–≤—É—é —á–∞—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
            import re
            age_match = re.search(r'\d+\s*–ª–µ—Ç', value)
            if age_match:
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é —á–∏—Å–ª–æ–≤–æ–π —á–∞—Å—Ç–∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
                age_text = age_match.group()
                age_start = text.find(age_text, start)
                if age_start != -1:
                    age_end = age_start + len(age_text)
                    final_improved.append((age_start, age_end, entity_type, age_text))
                    continue
        
        final_improved.append((start, end, entity_type, value))
    
    return final_improved


def get_dataset_name(dataset_path: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É."""
    return Path(dataset_path).stem


def save_metrics_to_csv(metrics_by_type: Dict[str, Dict[str, float]], 
                       method_name: str, dataset_name: str, output_dir: Path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –≤ CSV —Ñ–∞–π–ª."""
    # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
    safe_method_name = method_name.replace(" ", "_").replace("/", "_")
    filename = f"{dataset_name}_{safe_method_name}_metrics.csv"
    filepath = output_dir / filename
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['entity_type', 'precision', 'recall', 'f1', 'tp', 'fp', 'fn', 'total_true', 'total_pred', 'class_proportion']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        for entity_type, metrics in metrics_by_type.items():
            row = {'entity_type': entity_type}
            row.update(metrics)
            writer.writerow(row)
    
    print(f"üíæ –ú–µ—Ç—Ä–∏–∫–∏ {method_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")


def save_combined_metrics_to_csv(all_results: Dict[str, Dict[str, Dict[str, float]]], 
                                dataset_name: str, output_dir: Path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤—Å–µ—Ö –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ –æ–¥–∏–Ω CSV —Ñ–∞–π–ª."""
    filename = f"{dataset_name}_combined_metrics.csv"
    filepath = output_dir / filename
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['method', 'entity_type', 'precision', 'recall', 'f1', 'tp', 'fp', 'fn', 'total_true', 'total_pred', 'class_proportion']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        for method_name, metrics_by_type in all_results.items():
            for entity_type, metrics in metrics_by_type.items():
                row = {
                    'method': method_name,
                    'entity_type': entity_type
                }
                row.update(metrics)
                writer.writerow(row)
    
    print(f"üíæ –°–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")


def validate_anonymizer(anonymizer, name: str, test_data: List[Dict], verbose: bool = False) -> Tuple[Dict[str, Any], Dict[str, Dict[str, float]]]:
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –æ–¥–∏–Ω –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º."""
    if verbose:
        print(f"\nüîç –í–∞–ª–∏–¥–∞—Ü–∏—è {name}...")
    else:
        print(f"–í–∞–ª–∏–¥–∞—Ü–∏—è {name}...")
    
    start_time = time.time()
    
    all_true_entities = []
    all_predicted_entities = []
    processing_times = []
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–±–∞—Ä–∞
    if TQDM_AVAILABLE:
        iterator = tqdm(enumerate(test_data), total=len(test_data), desc=f"{name}", leave=False)
    else:
        iterator = enumerate(test_data)
    
    for i, example in iterator:
        text = example['text']
        true_entities = example['entities']
        
        # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        process_start = time.time()
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
            extracted = anonymizer.extract_entities(text)
            
            # –£–ª—É—á—à–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
            improved_extracted = improve_extracted_entities(extracted, text)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            predicted_entities = []
            for start, end, entity_type, value in improved_extracted:
                predicted_entities.append({
                    'start': start,
                    'end': end,
                    'type': entity_type,
                    'value': value
                })
            
            all_true_entities.extend(true_entities)
            all_predicted_entities.extend(predicted_entities)
            
        except Exception as e:
            error_emoji = "‚ùå " if verbose else ""
            print(f"{error_emoji}–û—à–∏–±–∫–∞ –≤ –ø—Ä–∏–º–µ—Ä–µ {i+1}: {e}")
            continue
        
        process_time = time.time() - process_start
        processing_times.append(process_time)
    
    total_time = time.time() - start_time
    
    # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (—Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    overall_metrics = calculate_metrics(all_true_entities, all_predicted_entities)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º (—Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é) 
    metrics_by_type = calculate_metrics_by_type(all_true_entities, all_predicted_entities)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ –æ–±—â–∏–º –º–µ—Ç—Ä–∏–∫–∞–º
    overall_metrics.update({
        'total_time': total_time,
        'avg_time_per_example': sum(processing_times) / len(processing_times) if processing_times else 0,
        'examples_processed': len(processing_times)
    })
    
    return overall_metrics, metrics_by_type


def print_results(results: Dict[str, Dict[str, Any]], all_detailed_results: Dict[str, Dict[str, Dict[str, float]]], verbose: bool = False):
    """–í—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–æ–Ω—Å–æ–ª—å."""
    print("\n" + "="*80)
    if verbose:
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ê–õ–ò–î–ê–¶–ò–ò (–°–¢–†–û–ì–ò–ï –ú–ï–¢–†–ò–ö–ò)")
    else:
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ê–õ–ò–î–ê–¶–ò–ò (–°–¢–†–û–ì–ò–ï –ú–ï–¢–†–ò–ö–ò)")
    print("="*80)
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
    print(f"{'–ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä':<25} {'Precision':<10} {'Recall':<10} {'F1':<10} {'–í—Ä–µ–º—è (—Å)':<12}")
    print("-" * 80)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ F1 score
    sorted_results = sorted(results.items(), key=lambda x: x[1]['f1'], reverse=True)
    
    for name, metrics in sorted_results:
        print(f"{name:<25} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} "
              f"{metrics['f1']:<10.3f} {metrics['total_time']:<12.2f}")
    
    print("-" * 80)
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ª—É—á—à–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
    best_name, best_metrics = sorted_results[0]
    best_emoji = "üèÜ " if verbose else ""
    print(f"\n{best_emoji}–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_name}")
    print(f"   F1-Score: {best_metrics['f1']:.3f}")
    print(f"   Precision: {best_metrics['precision']:.3f}")
    print(f"   Recall: {best_metrics['recall']:.3f}")
    print(f"   TP: {best_metrics['tp']}, FP: {best_metrics['fp']}, FN: {best_metrics['fn']}")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: {best_metrics['avg_time_per_example']*1000:.1f}–º—Å")
    
    # –î–æ–±–∞–≤–ª—è–µ–º macro –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞
    print("\n" + "="*80)
    if verbose:
        print("üìä MACRO –ú–ï–¢–†–ò–ö–ò –î–õ–Ø –ö–ê–ñ–î–û–ì–û –ú–ï–¢–û–î–ê")
    else:
        print("MACRO –ú–ï–¢–†–ò–ö–ò –î–õ–Ø –ö–ê–ñ–î–û–ì–û –ú–ï–¢–û–î–ê")
    print("="*80)
    
    print(f"{'–ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä':<25} {'Macro P':<10} {'Macro R':<10} {'Macro F1':<10}")
    print("-" * 80)
    
    for name, metrics in sorted_results:
        if name in all_detailed_results and 'ALL_MACRO' in all_detailed_results[name]:
            macro_metrics = all_detailed_results[name]['ALL_MACRO']
            print(f"{name:<25} {macro_metrics['precision']:<10.3f} {macro_metrics['recall']:<10.3f} "
                  f"{macro_metrics['f1']:<10.3f}")
        else:
            print(f"{name:<25} {'N/A':<10} {'N/A':<10} {'N/A':<10}")
    
    print("-" * 80)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser(
        description="–ü—Ä–æ—Å—Ç–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python validate_simple.py --dataset test_dataset.json
  python validate_simple.py --dataset data/generated/deepseek_v3_dataset_manual_final.json --verbose
  python validate_simple.py -d /path/to/custom_dataset.json -v
        """
    )
    
    parser.add_argument(
        '--dataset', '-d',
        type=str,
        required=True,
        help='–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏.'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        default=False,
        help='–í–∫–ª—é—á–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ —Å —ç–º–æ–¥–∑–∏ –∏ –¥–µ—Ç–∞–ª—è–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ—Ç–∫–ª—é—á–µ–Ω)'
    )
    
    args = parser.parse_args()
    
    startup_emoji = "üöÄ " if args.verbose else ""
    print(f"{startup_emoji}–ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤")
    
    folder_emoji = "üìÅ " if args.verbose else ""
    print(f"{folder_emoji}–†–∞–±–æ—á–∞—è –ø–∞–ø–∫–∞: {Path.cwd()}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_data = load_test_dataset(args.dataset)
    if not test_data:
        error_emoji = "‚ùå " if args.verbose else ""
        print(f"{error_emoji}–ù–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
        return
    
    list_emoji = "üìã " if args.verbose else ""
    print(f"{list_emoji}–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_name = get_dataset_name(args.dataset)
    chart_emoji = "üìä " if args.verbose else ""
    print(f"{chart_emoji}–ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_name}")
    
    # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–æ–≤
    output_dir = Path("data/reports")
    
    # –°–ø–∏—Å–æ–∫ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    anonymizers = {
        "RegExp Baseline": RegExpBaselineAnonymizer(),
        "Natasha Enhanced": EnhancedNatashaAnonymizer(),
        "SpaCy Extended": SpacyAnonymizer(),
        "Transformers BERT": BertAnonymizer(),
        "Hybrid Advanced": HybridAdvancedAnonymizer(verbose=args.verbose),
    }
    
    target_emoji = "üéØ " if args.verbose else ""
    print(f"{target_emoji}–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(anonymizers)} –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤")
    
    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä
    results = {}
    all_detailed_results = {}
    
    for name, anonymizer in anonymizers.items():
        try:
            overall_metrics, metrics_by_type = validate_anonymizer(anonymizer, name, test_data, args.verbose)
            results[name] = overall_metrics
            all_detailed_results[name] = metrics_by_type
            
            check_emoji = "‚úÖ " if args.verbose else ""
            print(f"{check_emoji}{name}: F1={overall_metrics['f1']:.3f}, P={overall_metrics['precision']:.3f}, R={overall_metrics['recall']:.3f}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π CSV
            save_metrics_to_csv(metrics_by_type, name, dataset_name, output_dir)
            
        except Exception as e:
            error_emoji = "‚ùå " if args.verbose else ""
            print(f"{error_emoji}–û—à–∏–±–∫–∞ –≤ {name}: {e}")
            continue
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    if all_detailed_results:
        save_combined_metrics_to_csv(all_detailed_results, dataset_name, output_dir)
    
    # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if results:
        print_results(results, all_detailed_results, args.verbose)
        folder_emoji = "üìÅ " if args.verbose else ""
        print(f"\n{folder_emoji}–û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É: {output_dir}")
        file_emoji = "üìÑ " if args.verbose else ""
        print(f"{file_emoji}–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {len(anonymizers) + 1}")
    else:
        error_emoji = "‚ùå " if args.verbose else ""
        print(f"{error_emoji}–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏")


if __name__ == "__main__":
    main() 