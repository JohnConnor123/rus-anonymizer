#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è DeepPavlov –º–æ–¥–µ–ª–µ–π.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª–µ–µ —É–º–Ω—É—é –ª–æ–≥–∏–∫—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π.
"""

import json
import time
import sys
import argparse
import csv
from pathlib import Path
from typing import Dict, List, Tuple, Any, Set
from collections import defaultdict

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# –ò–º–ø–æ—Ä—Ç –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–≥–æ –º–æ–¥—É–ª—è –º–µ—Ç—Ä–∏–∫
from utils.metrics import calculate_metrics, calculate_metrics_by_type

try:
    from anonymizers.deeppavlov_ner.deeppavlov_anonymizer import DeepPavlovAnonymizer
    DEEPPAVLOV_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå DeepPavlov –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    DEEPPAVLOV_AVAILABLE = False


def load_test_dataset(filename: str = "test_dataset.json") -> List[Dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç."""
    test_file = project_root / "data" / filename
    
    try:
        with open(test_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª {test_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return []


def merge_person_entities(entities: List[Tuple[int, int, str, str]]) -> List[Tuple[int, int, str, str]]:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ PERSON –≤ –æ–¥–Ω—É."""
    if not entities:
        return entities
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏
    sorted_entities = sorted(entities, key=lambda x: x[0])
    merged = []
    
    i = 0
    while i < len(sorted_entities):
        start, end, entity_type, value = sorted_entities[i]
        
        if entity_type == 'PERSON':
            # –ò—â–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ PERSON —Å—É—â–Ω–æ—Å—Ç–∏
            merged_start = start
            merged_end = end
            merged_value = value
            
            j = i + 1
            while j < len(sorted_entities):
                next_start, next_end, next_type, next_value = sorted_entities[j]
                
                # –ï—Å–ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—É—â–Ω–æ—Å—Ç—å —Ç–æ–∂–µ PERSON –∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ä—è–¥–æ–º (—Å —É—á–µ—Ç–æ–º –ø—Ä–æ–±–µ–ª–æ–≤)
                if (next_type == 'PERSON' and 
                    next_start <= merged_end + 3):  # +3 –¥–ª—è —É—á–µ—Ç–∞ –ø—Ä–æ–±–µ–ª–æ–≤
                    merged_end = next_end
                    merged_value = merged_value + " " + next_value if not merged_value.endswith(next_value) else merged_value
                    j += 1
                else:
                    break
            
            merged.append((merged_start, merged_end, entity_type, merged_value.strip()))
            i = j
        else:
            merged.append((start, end, entity_type, value))
            i += 1
    
    return merged


def improve_age_entities(entities: List[Tuple[int, int, str, str]], text: str) -> List[Tuple[int, int, str, str]]:
    """–£–ª—É—á—à–∞–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–∞ - —É–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞."""
    improved = []
    
    for start, end, entity_type, value in entities:
        if entity_type == 'AGE':
            # –ò—â–µ–º —á–∏—Å–ª–æ–≤—É—é —á–∞—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
            import re
            age_match = re.search(r'\d+\s*–ª–µ—Ç', value)
            if age_match:
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é —á–∏—Å–ª–æ–≤–æ–π —á–∞—Å—Ç–∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
                age_text = age_match.group()
                age_start = text.find(age_text, start)
                if age_start != -1:
                    age_end = age_start + len(age_text)
                    improved.append((age_start, age_end, entity_type, age_text))
                    continue
        
        improved.append((start, end, entity_type, value))
    
    return improved


def get_dataset_name(dataset_path: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É."""
    return Path(dataset_path).stem


def save_metrics_to_csv(metrics_by_type: Dict[str, Dict[str, float]], 
                       method_name: str, dataset_name: str, output_dir: Path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –≤ CSV —Ñ–∞–π–ª."""
    # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
    safe_method_name = method_name.replace(" ", "_").replace("/", "_")
    filename = f"{dataset_name}_{safe_method_name}_metrics.csv"
    filepath = output_dir / filename
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['entity_type', 'precision', 'recall', 'f1', 'tp', 'fp', 'fn', 'total_true', 'total_pred', 'class_proportion']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        for entity_type, metrics in metrics_by_type.items():
            row = {'entity_type': entity_type}
            row.update(metrics)
            writer.writerow(row)
    
    print(f"üíæ –ú–µ—Ç—Ä–∏–∫–∏ {method_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")


def update_combined_metrics_csv(metrics_by_type: Dict[str, Dict[str, float]], 
                               method_name: str, dataset_name: str, output_dir: Path) -> bool:
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É, –¥–æ–±–∞–≤–ª—è—è –¥–∞–Ω–Ω—ã–µ DeepPavlov –µ—Å–ª–∏ –∏—Ö —Ç–∞–º –Ω–µ—Ç.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ, False –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –µ—Å—Ç—å.
    """
    combined_filename = f"{dataset_name}_combined_metrics.csv"
    combined_filepath = output_dir / combined_filename
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
    if not combined_filepath.exists():
        print(f"‚ö†Ô∏è  –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ {combined_filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return False
    
    # –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    existing_data = []
    with open(combined_filepath, 'r', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            existing_data.append(row)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–∞—à–µ–≥–æ –º–µ—Ç–æ–¥–∞
    existing_methods = {row['method'] for row in existing_data}
    if method_name in existing_methods:
        print(f"‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –¥–ª—è {method_name} —É–∂–µ –µ—Å—Ç—å –≤ —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ")
        return False
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ
    new_rows = []
    for entity_type, metrics in metrics_by_type.items():
        row = {
            'method': method_name,
            'entity_type': entity_type
        }
        row.update(metrics)
        new_rows.append(row)
    
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    all_data = existing_data + new_rows
    with open(combined_filepath, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['method', 'entity_type', 'precision', 'recall', 'f1', 'tp', 'fp', 'fn', 'total_true', 'total_pred', 'class_proportion']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        for row in all_data:
            writer.writerow(row)
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ {method_name} –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É {combined_filepath}")
    return True


def validate_deeppavlov(test_data: List[Dict], dataset_name: str) -> Tuple[Dict[str, Any], Dict[str, Dict[str, float]]]:
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç DeepPavlov –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º."""
    print("\nüîç –í–∞–ª–∏–¥–∞—Ü–∏—è DeepPavlov...")
    
    start_time = time.time()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä
    print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DeepPavlov...")
    start_init = time.time()
    
    try:
        anonymizer = DeepPavlovAnonymizer(aggressiveness=0.8)
        init_time = time.time() - start_init
        print(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {init_time:.2f}—Å")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        raise
    
    all_true_entities = []
    all_predicted_entities = []
    processing_times = []
    
    for i, example in enumerate(test_data):
        text = example['text']
        true_entities = example['entities']
        
        # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        process_start = time.time()
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
            extracted = anonymizer.extract_entities(text)
            
            # –£–ª—É—á—à–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
            improved_entities = merge_person_entities(extracted)
            improved_entities = improve_age_entities(improved_entities, text)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            predicted_entities = []
            for start, end, entity_type, value in improved_entities:
                predicted_entities.append({
                    'start': start,
                    'end': end,
                    'type': entity_type,
                    'value': value
                })
            
            all_true_entities.extend(true_entities)
            all_predicted_entities.extend(predicted_entities)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–∏–º–µ—Ä–µ {i+1}: {e}")
            continue
        
        process_time = time.time() - process_start
        processing_times.append(process_time)
    
    total_time = time.time() - start_time
    
    # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (—Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    overall_metrics = calculate_metrics(all_true_entities, all_predicted_entities)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º (—Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    metrics_by_type = calculate_metrics_by_type(all_true_entities, all_predicted_entities)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ –æ–±—â–∏–º –º–µ—Ç—Ä–∏–∫–∞–º
    overall_metrics.update({
        'total_time': total_time,
        'init_time': init_time,
        'avg_time_per_example': sum(processing_times) / len(processing_times) if processing_times else 0,
        'examples_processed': len(processing_times)
    })
    
    return overall_metrics, metrics_by_type


def analyze_deeppavlov_improved(dataset_path: str = None, update_pivot: bool = False):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–±–æ—Ç—ã DeepPavlov."""
    if not DEEPPAVLOV_AVAILABLE:
        print("‚ùå DeepPavlov –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return
    
    print("üß† –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è DeepPavlov NER")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if dataset_path:
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if not Path(dataset_path).exists():
            print(f"‚ùå –§–∞–π–ª {dataset_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
        
        with open(dataset_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å metadata
        if isinstance(data, dict) and 'dialogs' in data:
            print("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ó–∞–≥—Ä—É–∑–∫–∞...")
            
            source_dialogs = data.get('dialogs', [])
            test_data = []

            for dialog in source_dialogs:
                entities = []
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–≤–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø–æ–ª—è –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π: 'entities' –∏ 'personal_data'
                entities_source = dialog.get('entities', dialog.get('personal_data', []))
                
                for entity_data in entities_source:
                    # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—É—â–Ω–æ—Å—Ç—å
                    entity = {
                        'start': entity_data.get('start'),
                        'end': entity_data.get('end'),
                        'type': entity_data.get('type'),
                    }
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å—É—â–Ω–æ—Å—Ç–∏
                    if 'text' in entity_data:
                        entity['text'] = entity_data['text']
                    elif 'value' in entity_data:
                        entity['text'] = entity_data['value']
                    else:
                        entity['text'] = ''
                    
                    entities.append(entity)

                test_data.append({
                    'id': dialog.get('id', len(test_data) + 1),
                    'text': dialog.get('text', ''),
                    'entities': entities,
                    'has_pd': bool(entities),
                    'description': dialog.get('description', f"–î–∏–∞–ª–æ–≥ ID: {dialog.get('id')}")
                })
        else:
            test_data = data
    else:
        test_data = load_test_dataset()
    
    if not test_data:
        print("‚ùå –ù–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        return
    
    print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_name = get_dataset_name(dataset_path) if dataset_path else "test_dataset"
    print(f"üìä –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_name}")
    
    # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–æ–≤
    output_dir = Path("data/reports")
    
    try:
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º DeepPavlov
        overall_metrics, metrics_by_type = validate_deeppavlov(test_data, dataset_name)
        
        print(f"‚úÖ DeepPavlov: F1={overall_metrics['f1']:.3f}, P={overall_metrics['precision']:.3f}, R={overall_metrics['recall']:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π CSV
        save_metrics_to_csv(metrics_by_type, "DeepPavlov", dataset_name, output_dir)
        
        files_created = 1
        
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ñ–ª–∞–≥ update_pivot, –ø—ã—Ç–∞–µ–º—Å—è –æ–±–Ω–æ–≤–∏—Ç—å —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
        if update_pivot:
            print(f"\nüîÑ –ü–æ–ø—ã—Ç–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã...")
            if update_combined_metrics_csv(metrics_by_type, "DeepPavlov", dataset_name, output_dir):
                files_created = 2  # –û–±–Ω–æ–≤–∏–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ DeepPavlov: {e}")
        return
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 60)
    print("üìä –°–¢–†–û–ì–ò–ï –ú–ï–¢–†–ò–ö–ò (EXACT MATCH)")
    print("=" * 60)
    
    print(f"–í—Å–µ–≥–æ –æ–∂–∏–¥–∞–µ–º—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π: {overall_metrics['tp'] + overall_metrics['fn']}")
    print(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π: {overall_metrics['tp'] + overall_metrics['fp']}")
    print(f"–ü—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö: {overall_metrics['tp']}")
    print(f"")
    print(f"üéØ Precision: {overall_metrics['precision']:.3f} ({overall_metrics['precision']*100:.1f}%)")
    print(f"üéØ Recall: {overall_metrics['recall']:.3f} ({overall_metrics['recall']*100:.1f}%)")
    print(f"üéØ F1-Score: {overall_metrics['f1']:.3f} ({overall_metrics['f1']*100:.1f}%)")
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print(f"\n‚ö° –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨")
    print(f"–í—Ä–µ–º—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {overall_metrics['init_time']:.2f}—Å")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: {overall_metrics['avg_time_per_example']*1000:.1f}–º—Å")
    print(f"\nüìÅ –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É: {output_dir}")
    print(f"üìÑ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ —Å–æ–∑–¥–∞–Ω–æ/–æ–±–Ω–æ–≤–ª–µ–Ω–æ: {files_created}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser(
        description="–í–∞–ª–∏–¥–∞—Ü–∏—è DeepPavlov –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–∞",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python validate_deeppavlov.py
  python validate_deeppavlov.py --dataset test_dataset.json
  python validate_deeppavlov.py --dataset data/generated/deepseek_v3_dataset_manual_final.json
  python validate_deeppavlov.py -d /path/to/custom_dataset.json --update-pivot
        """
    )
    
    parser.add_argument(
        '--dataset', '-d',
        type=str,
        help='–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è test_dataset.json'
    )
    
    parser.add_argument(
        '--update-pivot',
        action='store_true',
        help='–ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –æ–±–Ω–æ–≤–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É, –¥–æ–±–∞–≤–∏–≤ –¥–∞–Ω–Ω—ã–µ DeepPavlov'
    )
    
    args = parser.parse_args()
    
    print("üöÄ –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è DeepPavlov –º–æ–¥–µ–ª–µ–π")
    print("=" * 50)
    
    analyze_deeppavlov_improved(args.dataset, args.update_pivot)


if __name__ == "__main__":
    main() 