"""
–ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Ö–æ–¥–æ–≤.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –±–µ–∑ DeepPavlov.
"""

from typing import List, Tuple, Optional, Callable, Dict, Any
import statistics

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å
from ..base import BaseAnonymizer

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä—ã (–±–µ–∑ DeepPavlov)
from ..natasha_enhanced.enhanced_natasha import EnhancedNatashaAnonymizer
from ..spacy_extended.spacy_anonymizer import SpacyAnonymizer  
from ..transformers_bert.bert_anonymizer import BertAnonymizer
from ..regexp_baseline.regexp_anonymizer import RegExpBaselineAnonymizer

# –£–±–∏—Ä–∞–µ–º –∏–º–ø–æ—Ä—Ç DeepPavlov
# from ..deeppavlov_ner.deeppavlov_anonymizer import DeepPavlovAnonymizer, DeepPavlovMultiModelAnonymizer


class HybridAdvancedAnonymizer(BaseAnonymizer):
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Ö–æ–¥–æ–≤.
    –ë–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è DeepPavlov –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.
    """
    
    def __init__(
        self,
        aggressiveness: float = 0.8,
        anonymization_func: Optional[Callable[[str, str], str]] = None,
        use_consensus: bool = True,
        consensus_threshold: float = 0.4
    ):
        super().__init__(aggressiveness, anonymization_func)
        
        self.use_consensus = use_consensus
        self.consensus_threshold = consensus_threshold
        
        # –°–ª–æ–≤–∞—Ä—å –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏ –∏—Ö –≤–µ—Å–æ–≤
        self.anonymizers = {}
        self.anonymizer_weights = {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä—ã
        self._init_anonymizers()
    
    def _init_anonymizers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ (–±–µ–∑ DeepPavlov)."""
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–∞...")
        
        # 1. Natasha Enhanced (–æ–±—ã—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ)
        try:
            self.anonymizers['natasha'] = EnhancedNatashaAnonymizer(
                aggressiveness=self.aggressiveness
            )
            self.anonymizer_weights['natasha'] = 1.0
            print("‚úÖ Natasha Enhanced –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Natasha: {e}")
        
        # 2. spaCy Extended (–º–æ–∂–µ—Ç –∏–º–µ—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –º–æ–¥–µ–ª—å—é)
        try:
            self.anonymizers['spacy'] = SpacyAnonymizer(
                aggressiveness=self.aggressiveness
            )
            self.anonymizer_weights['spacy'] = 1.1
            print("‚úÖ SpaCy Extended –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ SpaCy: {e}")
        
        # 3. BERT Transformers (–º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏)
        try:
            self.anonymizers['bert'] = BertAnonymizer(
                aggressiveness=self.aggressiveness
            )
            self.anonymizer_weights['bert'] = 1.3
            print("‚úÖ BERT Transformers –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ BERT: {e}")
        
        # 4. RegExp Baseline (–≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å)
        try:
            self.anonymizers['regexp'] = RegExpBaselineAnonymizer(
                aggressiveness=self.aggressiveness
            )
            self.anonymizer_weights['regexp'] = 0.7
            print("‚úÖ RegExp Baseline –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RegExp: {e}")
        
        print(f"üéØ –ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤ —Å {len(self.anonymizers)} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏")
        
        if not self.anonymizers:
            raise RuntimeError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–∞!")
    
    def extract_entities(self, text: str) -> List[Tuple[int, int, str, str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä—ã.
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ–Ω—Å–µ–Ω—Å—É—Å –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ.
        """
        if not self.anonymizers:
            raise RuntimeError("–ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä—ã –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        
        all_results = {}
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç –≤—Å–µ—Ö –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
        for name, anonymizer in self.anonymizers.items():
            try:
                entities = anonymizer.extract_entities(text)
                all_results[name] = entities
                print(f"üìä {name}: –Ω–∞–π–¥–µ–Ω–æ {len(entities)} —Å—É—â–Ω–æ—Å—Ç–µ–π")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ {name}: {e}")
                all_results[name] = []
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if self.use_consensus:
            return self._consensus_merge(all_results, text)
        else:
            return self._simple_merge(all_results)
    
    def _consensus_merge(self, all_results: Dict[str, List], text: str) -> List[Tuple[int, int, str, str]]:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞.
        –°—É—â–Ω–æ—Å—Ç—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—ë –Ω–∞—à–ª–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤.
        """
        entity_votes = {}  # (start, end, type) -> —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        # –°–æ–±–∏—Ä–∞–µ–º –≥–æ–ª–æ—Å–∞
        for anonymizer_name, entities in all_results.items():
            weight = self.anonymizer_weights.get(anonymizer_name, 1.0)
            
            for start, end, entity_type, value in entities:
                key = (start, end, entity_type)
                
                if key not in entity_votes:
                    entity_votes[key] = []
                
                entity_votes[key].append({
                    'source': anonymizer_name,
                    'weight': weight,
                    'value': value
                })
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–æ–Ω—Å–µ–Ω—Å—É—Å—É
        final_entities = []
        total_weight = sum(self.anonymizer_weights.values())
        
        for (start, end, entity_type), votes in entity_votes.items():
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—É–º–º–∞—Ä–Ω—ã–π –≤–µ—Å –≥–æ–ª–æ—Å–æ–≤
            vote_weight = sum(vote['weight'] for vote in votes)
            consensus_score = vote_weight / total_weight
            
            if consensus_score >= self.consensus_threshold:
                # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–æ—Ç —Å–∞–º–æ–≥–æ –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞)
                best_vote = max(votes, key=lambda x: x['weight'])
                
                final_entities.append((
                    start, end, entity_type, best_vote['value']
                ))
        
        # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
        final_entities = self._remove_overlapping(final_entities)
        
        print(f"ü§ù –ö–æ–Ω—Å–µ–Ω—Å—É—Å: {len(final_entities)} —Å—É—â–Ω–æ—Å—Ç–µ–π –ø—Ä–æ—à–ª–∏ –ø–æ—Ä–æ–≥ {self.consensus_threshold}")
        return sorted(final_entities, key=lambda x: x[0])
    
    def _simple_merge(self, all_results: Dict[str, List]) -> List[Tuple[int, int, str, str]]:
        """–ü—Ä–æ—Å—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —É–¥–∞–ª–µ–Ω–∏–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤."""
        all_entities = []
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for anonymizer_name, entities in all_results.items():
            for entity in entities:
                all_entities.append(entity)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
        unique_entities = self._remove_overlapping(all_entities)
        
        print(f"üîó –ü—Ä–æ—Å—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ: {len(unique_entities)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π")
        return sorted(unique_entities, key=lambda x: x[0])
    
    def _remove_overlapping(self, entities: List[Tuple[int, int, str, str]]) -> List[Tuple[int, int, str, str]]:
        """–£–¥–∞–ª—è–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —Å—É—â–Ω–æ—Å—Ç–∏."""
        if not entities:
            return entities
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–∞—á–∞–ª—É, –ø–æ—Ç–æ–º –ø–æ –¥–ª–∏–Ω–µ (—É–±—ã–≤–∞–Ω–∏–µ)
        sorted_entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))
        
        result = []
        for entity in sorted_entities:
            start, end = entity[0], entity[1]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏
            overlaps = False
            for existing in result:
                existing_start, existing_end = existing[0], existing[1]
                if not (end <= existing_start or start >= existing_end):
                    overlaps = True
                    break
            
            if not overlaps:
                result.append(entity)
        
        return result
    
    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–º –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º."""
        stats = {
            'total_anonymizers': len(self.anonymizers),
            'available_anonymizers': list(self.anonymizers.keys()),
            'weights': self.anonymizer_weights.copy(),
            'consensus_enabled': self.use_consensus,
            'consensus_threshold': self.consensus_threshold if self.use_consensus else None
        }
        
        return stats


class HybridAdvancedAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä–∞–±–æ—Ç—ã –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–∞."""
    
    def __init__(self):
        self.anonymizers = {}
        self._init_all_anonymizers()
    
    def _init_all_anonymizers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."""
        print("üî¨ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞...")
        
        # 1. Natasha Enhanced
        try:
            self.anonymizers['natasha'] = EnhancedNatashaAnonymizer(aggressiveness=0.8)
            print("‚úÖ Natasha Enhanced")
        except Exception as e:
            print(f"‚ùå Natasha Enhanced: {e}")
        
        # 2. spaCy Extended
        try:
            self.anonymizers['spacy'] = SpacyAnonymizer(aggressiveness=0.8)
            print("‚úÖ SpaCy Extended")
        except Exception as e:
            print(f"‚ùå SpaCy Extended: {e}")
        
        # 3. BERT Transformers
        try:
            self.anonymizers['bert'] = BertAnonymizer(aggressiveness=0.8)
            print("‚úÖ BERT Transformers")
        except Exception as e:
            print(f"‚ùå BERT Transformers: {e}")
        
        # 4. RegExp Baseline
        try:
            self.anonymizers['regexp'] = RegExpBaselineAnonymizer(aggressiveness=0.8)
            print("‚úÖ RegExp Baseline")
        except Exception as e:
            print(f"‚ùå RegExp Baseline: {e}")
        
        print(f"üìä –ì–æ—Ç–æ–≤–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(self.anonymizers)} –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤")
    
    def analyze_text(self, text: str) -> Dict[str, Any]:
        """–ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        results = {}
        
        for name, anonymizer in self.anonymizers.items():
            try:
                entities = anonymizer.extract_entities(text)
                results[name] = {
                    'entities': entities,
                    'count': len(entities),
                    'types': list(set(e[2] for e in entities))
                }
            except Exception as e:
                results[name] = {
                    'error': str(e),
                    'entities': [],
                    'count': 0,
                    'types': []
                }
        
        return results
    
    def compare_performance(self, test_texts: List[str]) -> Dict[str, Any]:
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ —Ç–µ–∫—Å—Ç–æ–≤."""
        performance = {}
        
        for name in self.anonymizers.keys():
            performance[name] = {
                'total_entities': 0,
                'avg_entities_per_text': 0,
                'entity_types': set(),
                'success_rate': 0,
                'errors': 0
            }
        
        for text in test_texts:
            for name, anonymizer in self.anonymizers.items():
                try:
                    entities = anonymizer.extract_entities(text)
                    performance[name]['total_entities'] += len(entities)
                    performance[name]['entity_types'].update(e[2] for e in entities)
                except Exception as e:
                    performance[name]['errors'] += 1
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        total_texts = len(test_texts)
        for name in performance.keys():
            perf = performance[name]
            perf['avg_entities_per_text'] = perf['total_entities'] / total_texts
            perf['success_rate'] = (total_texts - perf['errors']) / total_texts
            perf['entity_types'] = list(perf['entity_types'])
        
        return performance 